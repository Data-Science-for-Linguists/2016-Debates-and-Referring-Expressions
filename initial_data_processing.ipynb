{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2016 Election Project \n",
    "\n",
    "This notebook is intended to document my data processing throughout this project. I'll be poking around and modifying my data in this file. The data I am starting out with are transcripts of the presidential debates from the 2016 US Election between Hillary Clinton and Donald Trump. The transcripts were taken from UCSB's American Presidency Project, and the citation for each of the transcripts will be included both above, and as a part of the data I process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, I have three transcripts from the three presidential debates.\n",
    "\n",
    "Presidential Candidates Debates: \"Presidential Debate at the University of Nevada in Las Vegas,\" October 19, 2016. Online by Gerhard Peters and John T. Woolley, The American Presidency Project. http://www.presidency.ucsb.edu/ws/?pid=119039.\n",
    "\n",
    "Presidential Candidates Debates: \"Presidential Debate at Washington University in St. Louis, Missouri,\" October 9, 2016. Online by Gerhard Peters and John T. Woolley, The American Presidency Project. http://www.presidency.ucsb.edu/ws/?pid=119038.\n",
    "\n",
    "Presidential Candidates Debates: \"Presidential Debate at Hofstra University in Hempstead, New York,\" September 26, 2016. Online by Gerhard Peters and John T. Woolley, The American Presidency Project. http://www.presidency.ucsb.edu/ws/?pid=118971.\n",
    "\n",
    "\n",
    "**I might use other speeches as well, but I think I will use their individual speeches AFTER they have both been chosen as their party's candidates. Since I'll be adding manual RE annotation, I worry about using too many files** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10-19-16.txt',\n",
       " '10-9-16.txt',\n",
       " '9-26-16.txt',\n",
       " '1-17-16_dem.txt',\n",
       " '1-25-16_dem.txt',\n",
       " '10-13-15_dem.txt',\n",
       " '11-14-15_dem.txt',\n",
       " '12-19-15_dem.txt',\n",
       " '2-11-16_dem.txt',\n",
       " '2-4-16_dem.txt',\n",
       " '3-6-16_dem.txt',\n",
       " '3-9-16_dem.txt',\n",
       " '4-14-16_dem.txt',\n",
       " '1-14-16_rep.txt',\n",
       " '1-28-16_rep.txt',\n",
       " '10-28-15_rep.txt',\n",
       " '11-10-15_rep.txt',\n",
       " '12-15-15_rep.txt',\n",
       " '2-13-16_rep.txt',\n",
       " '2-25-16_rep.txt',\n",
       " '2-6-16_rep.txt',\n",
       " '3-10-16_rep.txt',\n",
       " '3-3-16_rep.txt',\n",
       " '8-6-15_rep.txt',\n",
       " '9-16-15_rep.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/Users/Paige/Documents/Data_Science/2016-Election-Project/data/Debates/general/')\n",
    "files = glob.glob(\"*.txt\")\n",
    "os.chdir('/Users/Paige/Documents/Data_Science/2016-Election-Project/data/Debates/dem/')\n",
    "files.extend(glob.glob('*.txt'))\n",
    "os.chdir('/Users/Paige/Documents/Data_Science/2016-Election-Project/data/Debates/rep/')\n",
    "files.extend(glob.glob('*.txt'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '10-19-16.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d122fad4c734>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtranscripts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '10-19-16.txt'"
     ]
    }
   ],
   "source": [
    "#I'm creating a list where each entry in the list is a transcript\n",
    "transcripts = []\n",
    "for f in files:\n",
    "    fi = open(f, 'r')\n",
    "    txt = fi.read()\n",
    "    fi.close\n",
    "    transcripts.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c266ab8ec1b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscripts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(transcripts[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words0= nltk.word_tokenize(transcripts[0])\n",
    "len(words0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents0= nltk.sent_tokenize(transcripts[0])\n",
    "len(sents0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcripts[1][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1= nltk.word_tokenize(transcripts[1])\n",
    "len(words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents1= nltk.sent_tokenize(transcripts[1])\n",
    "len(sents1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcripts[2][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2= nltk.word_tokenize(transcripts[2])\n",
    "len(words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents2= nltk.sent_tokenize(transcripts[2])\n",
    "len(sents2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first debate has 20719 words and 1292 sentences including markers for who is speaking. The second debate has 19613 words and 1246 sentences including markers for who is speaking. The third debate has 20372 words and 1236 sentences including markers for who is speaking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that we need to do some clean up. What I would eventually like to end up with is a dataframe where the columns are Debate, Date, Source, Speaker, Sents, where the Sents are in the order of their speech. For now, I will keep the speech/questions of the moderators, becuase it might be interesting to compare the referring expressions *they* use for the candidates vs what the cadidates use for each other.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I want to split large chunks of the transcript based on who is speaking.\n",
    "#Since the transcript data has a pretty standardized fomat (The speaker is in all caps followed by a colon)\n",
    "#I can add a marker to each of these sections, and split the data on that marker\n",
    "\n",
    "speaker_split = []\n",
    "\n",
    "for txt in transcripts:\n",
    "    speaker_split.append(txt.replace(\"CLINTON:\", \"#$&CLINTON*:\").replace(\"TRUMP:\", \"#$&TRUMP*:\").replace(\"WALLACE:\", \"#$&WALLACE*:\").replace(\"COOPER:\", \"#$&COOPER*:\").replace(\"RADDATZ:\", \"#$&RADDATZ*:\").replace(\"HOLT:\", \"#$&HOLT*:\").replace(\"PARTICIPANTS:\", \"#$&PARTICIPANTS*:\").replace(\"MODERATOR:\", \"#$&MODERATOR*:\").replace(\"MODERATORS:\", \"#$&MODERATORS*:\").replace(\"\\n\", \" \"))\n",
    "\n",
    "speaker_split = [txt.strip().split(\"#$&\") for txt in speaker_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_split[0][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating three separate lists of split speech by speaker for each debate\n",
    "debate3 = speaker_split[0]\n",
    "debate2 = speaker_split[1]\n",
    "debate1 = speaker_split[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting the SPEAKER: from the speech\n",
    "debate3 = [txt.split(\"*:\") for txt in debate3]\n",
    "debate2 = [txt.split(\"*:\") for txt in debate2]\n",
    "debate1 = [txt.split(\"*:\") for txt in debate1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debate3 #We can see that we need to remove the empty list at the beginning, and strip all of the entries\n",
    "debate3.remove([''])\n",
    "debate3[:4]\n",
    "#We'll strip the entries when they're in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debate2.remove([''])\n",
    "debate1.remove([''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debate3df = pd.DataFrame(debate3)\n",
    "#I want to add a column of the source of the transcript for each dataframe\n",
    "#I'm adding these columns with all the same value because I will eventually combine the \n",
    "#dataframes from all three debates, and this information will be important then\n",
    "\n",
    "debate3df['Source'] = 'http://www.presidency.ucsb.edu/ws/?pid=119039'\n",
    "debate3df['Debate'] = '3' \n",
    "debate3df['Location'] = 'University of Nevada in Las Vegas'\n",
    "debate3df['Date'] = '10/19/16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debate3df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Renaming the first two columns\n",
    "debate3df.columns = ['Speaker', 'Speech', 'Source', 'Debate', 'Location', 'Date']\n",
    "\n",
    "#Stripping the text in the Speaker and Speech columns\n",
    "debate3df['Speaker'] = debate3df['Speaker'].apply(lambda x: x.strip())\n",
    "debate3df['Speech'] = debate3df['Speech'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debate3df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reorganize the order of the columns\n",
    "debate3df = debate3df[['Location', 'Date', 'Debate', 'Source', 'Speaker', 'Speech']]\n",
    "\n",
    "#Drop these first two rows, because they are not speech information\n",
    "debate3df.drop(0, inplace=True)\n",
    "debate3df.drop(1, inplace=True)\n",
    "\n",
    "debate3df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is the same processing for debate2df\n",
    "debate2df = pd.DataFrame(debate2)\n",
    "debate2df['Source'] = 'http://www.presidency.ucsb.edu/ws/?pid=119038'\n",
    "debate2df['Debate'] = '2' \n",
    "debate2df['Location'] = 'Washington University in St. Louis, Missouri'\n",
    "debate2df['Date'] = '10/9/16'\n",
    "\n",
    "#Renaming the first two columns\n",
    "debate2df.columns = ['Speaker', 'Speech', 'Source', 'Debate', 'Location', 'Date']\n",
    "\n",
    "#Stripping the text in the Speaker and Speech columns\n",
    "debate2df['Speaker'] = debate2df['Speaker'].apply(lambda x: x.strip())\n",
    "debate2df['Speech'] = debate2df['Speech'].apply(lambda x: x.strip())\n",
    "\n",
    "#Reorganize the order of the columns\n",
    "debate2df = debate2df[['Location', 'Date', 'Debate', 'Source', 'Speaker', 'Speech']]\n",
    "\n",
    "#Drop these first two rows, because they are not speech information\n",
    "debate2df.drop(0, inplace=True)\n",
    "debate2df.drop(1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is the same processing for debate1df\n",
    "debate1df = pd.DataFrame(debate1)\n",
    "debate1df['Source'] = 'http://www.presidency.ucsb.edu/ws/?pid=118971'\n",
    "debate1df['Debate'] = '1' \n",
    "debate1df['Location'] = 'Hofstra University in Hempstead, New York'\n",
    "debate1df['Date'] = '9/26/16'\n",
    "\n",
    "#Renaming the first two columns\n",
    "debate1df.columns = ['Speaker', 'Speech', 'Source', 'Debate', 'Location', 'Date']\n",
    "\n",
    "#Stripping the text in the Speaker and Speech columns\n",
    "debate1df['Speaker'] = debate1df['Speaker'].apply(lambda x: x.strip())\n",
    "debate1df['Speech'] = debate1df['Speech'].apply(lambda x: x.strip())\n",
    "\n",
    "#Reorganize the order of the columns\n",
    "debate1df = debate1df[['Location', 'Date', 'Debate', 'Source', 'Speaker', 'Speech']]\n",
    "\n",
    "#Drop these first two rows, because they are not speech information\n",
    "debate1df.drop(0, inplace=True)\n",
    "debate1df.drop(1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debate1df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debate2df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Currently, the entries in this data frame are split into chunks of who is speaking. I think I might want each entry in the data frame to be a sentence instead. I'm going to make another dataframe where each row is information about one sentence. I'm going to keep both dataframes in case I decide one would be more helpful than the other later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debate3[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debate3_sent = []\n",
    "for chunk in debate3:\n",
    "    sents = nltk.sent_tokenize(chunk[1])\n",
    "    for sent in sents:\n",
    "        debate3_sent.append([chunk[0], sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3_sents = pd.DataFrame(debate3_sent)\n",
    "df3_sents['Source'] = 'http://www.presidency.ucsb.edu/ws/?pid=119039'\n",
    "df3_sents['Debate'] = '3' \n",
    "df3_sents['Location'] = 'University of Nevada in Las Vegas'\n",
    "df3_sents['Date'] = '10/19/16'\n",
    "\n",
    "#Renaming the first two columns\n",
    "df3_sents.columns = ['Speaker', 'Speech', 'Source', 'Debate', 'Location', 'Date']\n",
    "\n",
    "#Stripping the text in the Speaker and Speech columns\n",
    "df3_sents['Speaker'] = df3_sents['Speaker'].apply(lambda x: x.strip())\n",
    "df3_sents['Speech'] = df3_sents['Speech'].apply(lambda x: x.strip())\n",
    "\n",
    "#Reorganize the order of the columns\n",
    "df3_sents = df3_sents[['Location', 'Date', 'Debate', 'Source', 'Speaker', 'Speech']]\n",
    "\n",
    "#Drop these first two rows, because they are not speech information\n",
    "df3_sents.drop(0, inplace=True)\n",
    "df3_sents.drop(1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_sents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The same for debate 1\n",
    "debate1_sent = []\n",
    "for chunk in debate1:\n",
    "    sents = nltk.sent_tokenize(chunk[1])\n",
    "    for sent in sents:\n",
    "        debate1_sent.append([chunk[0], sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_sents = pd.DataFrame(debate1_sent)\n",
    "df1_sents['Source'] = 'http://www.presidency.ucsb.edu/ws/?pid=118971'\n",
    "df1_sents['Debate'] = '1' \n",
    "df1_sents['Location'] = 'Hofstra University in Hempstead, New York'\n",
    "df1_sents['Date'] = '9/26/16'\n",
    "\n",
    "#Renaming the first two columns\n",
    "df1_sents.columns = ['Speaker', 'Speech', 'Source', 'Debate', 'Location', 'Date']\n",
    "\n",
    "#Stripping the text in the Speaker and Speech columns\n",
    "df1_sents['Speaker'] = df1_sents['Speaker'].apply(lambda x: x.strip())\n",
    "df1_sents['Speech'] = df1_sents['Speech'].apply(lambda x: x.strip())\n",
    "\n",
    "#Reorganize the order of the columns\n",
    "df1_sents = df1_sents[['Location', 'Date', 'Debate', 'Source', 'Speaker', 'Speech']]\n",
    "\n",
    "#Drop these first two rows, because they are not speech information\n",
    "df1_sents.drop(0, inplace=True)\n",
    "df1_sents.drop(1, inplace=True)\n",
    "df1_sents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The same for debate 2\n",
    "debate2_sent = []\n",
    "for chunk in debate2:\n",
    "    sents = nltk.sent_tokenize(chunk[1])\n",
    "    for sent in sents:\n",
    "        debate2_sent.append([chunk[0], sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_sents = pd.DataFrame(debate2_sent)\n",
    "df2_sents['Source'] = 'http://www.presidency.ucsb.edu/ws/?pid=119038'\n",
    "df2_sents['Debate'] = '2' \n",
    "df2_sents['Location'] = 'Washington University in St. Louis, Missouri'\n",
    "df2_sents['Date'] = '10/9/16'\n",
    "\n",
    "#Renaming the first two columns\n",
    "df2_sents.columns = ['Speaker', 'Speech', 'Source', 'Debate', 'Location', 'Date']\n",
    "\n",
    "#Stripping the text in the Speaker and Speech columns\n",
    "df2_sents['Speaker'] = df2_sents['Speaker'].apply(lambda x: x.strip())\n",
    "df2_sents['Speech'] = df2_sents['Speech'].apply(lambda x: x.strip())\n",
    "\n",
    "#Reorganize the order of the columns\n",
    "df2_sents = df2_sents[['Location', 'Date', 'Debate', 'Source', 'Speaker', 'Speech']]\n",
    "\n",
    "#Drop these first two rows, because they are not speech information\n",
    "df2_sents.drop(0, inplace=True)\n",
    "df2_sents.drop(1, inplace=True)\n",
    "df2_sents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now I have 2 dataframes for each debate. One is a dataframe where each row is information on a chunk of speech, and the other is a dataframe where each row is information on a particular sentence. Both the chunks and sentences are in the order in which they were spoken. Now I'm going to export these dataframes to CSV files and annotate them for referring expressions manually.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1_sents.to_csv('debate1_sents.csv')\n",
    "df2_sents.to_csv('debate2_sents.csv')\n",
    "df3_sents.to_csv('debate3_sents.csv')\n",
    "debate1df.to_csv('debate1.csv')\n",
    "debate2df.to_csv('debate2.csv')\n",
    "debate3df.to_csv('debate3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I have manually annotated the 3 debatex_sents.csv files for the name/expression Donald Trump, Hillary Clinton, and the moderators used to refer to any of the candidates or another person.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1_re = pd.read_csv('debate1_sents_first.csv', encoding = 'latin1')\n",
    "df1_re = df1_re.drop('Unnamed: 0', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_re.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2_re = pd.read_csv('debate2_sents_first.csv', encoding = 'latin1')\n",
    "df2_re = df2_re.drop('Unnamed: 0', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_re.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3_re = pd.read_csv('debate3_sents_first.csv', encoding = 'latin1')\n",
    "df3_re = df3_re.drop('Unnamed: 0', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3_re.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_re_only = df1_re.loc[df1_re.RE.notnull(), :]\n",
    "df1_re_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1_re_only) #How many RE's/names were there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many times did each speaker refer to someone else?\n",
    "df1_re_only.groupby(['Speaker']).count()['RE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_re_only = df2_re.loc[df2_re.RE.notnull(), :]\n",
    "df2_re_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2_re_only) #How many RE's/names were there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many times did each speaker refer to someone else?\n",
    "df2_re_only.groupby(['Speaker']).count()['RE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_re_only = df3_re.loc[df3_re.RE.notnull(), :]\n",
    "df3_re_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df3_re_only) #How many RE's/names were there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many times did each speaker refer to someone else?\n",
    "df3_re_only.groupby(['Speaker']).count()['RE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the first debate, I found 149 names and referring expressions. In the second debate I found 191 names and referring expressions. In the third debate I found 223 reffering expressions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#As a sample of my data, I will be uploading csvs that only contain rows with RE's.\n",
    "df1_re_only.to_csv('/Users/Paige/Documents/Data_Science/2016-Election-Project/data_samples/df1_re_only.csv')\n",
    "df2_re_only.to_csv('/Users/Paige/Documents/Data_Science/2016-Election-Project/data_samples/df2_re_only.csv')\n",
    "df3_re_only.to_csv('/Users/Paige/Documents/Data_Science/2016-Election-Project/data_samples/df3_re_only.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharing Plan\n",
    "\n",
    "The only information I had about copyright from the American Presidency Project was a citation. I believe since I modified the data and included citations for each debate and in all of my data frames I include the source url of the transcript for each sentence, it is ok for me to share my modified version of the data. I would like to make my code completely available to the world and the members of this class. I do need to keep look into more information regarding sharing my data. For now, I am going to keep only sharing samples of the data frames that I create, but for the reasons I mentioned before, I believe I am able to share all of my data. With my added linguistic annotation, I would share my data with the Creative Commons Attribution Share Alike 4.0 license. If I can find more licensing information and find out that the members of the American Presidency Project disapprove of my distribution of a modification of their data even though it is cited, I will keep my data private and only share my analysis of the data. I contacted them and hope to hear back soon."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
