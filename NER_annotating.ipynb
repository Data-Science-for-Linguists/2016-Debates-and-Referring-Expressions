{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2016 Election Project \n",
    "\n",
    "This notebook is intended to document NER annotation of my data throughout this project. The data I am starting out with are transcripts of the presidential debates from the 2016 US Election- the 10 Democratic primary debates, the 12 Republican primary debates, and the debates for the general election between Hillary Clinton and Donald Trump. The transcripts were taken from UCSB's American Presidency Project. The citations for these transcripts can be found in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, this data needs to be cleaned up to remove SPEAKER: labels. Even though the participant and moderator information at the top include important entities, I'm want to remove it in order to have the tokenized sentences match my preexisting dataframes. Because the number of participants and moderators varies for each transcript, I manually created new files in the removed folder in transcripts/ with these portions removed. That is what I'll be processing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1-14-16_rep_removed.txt', '1-17-16_dem_removed.txt', '1-25-16_dem_removed.txt', '1-28-16_rep_removed.txt', '10-13-15_dem_removed.txt', '10-19-16_removed.txt', '10-28-15_rep_removed.txt', '10-9-16_removed.txt', '11-10-15_rep_removed.txt', '11-14-15_dem_removed.txt', '12-15-15_rep_removed.txt', '12-19-15_dem_removed.txt', '2-11-16_dem_removed.txt', '2-13-16_rep_removed.txt', '2-25-16_rep_removed.txt', '2-4-16_dem_removed.txt', '2-6-16_rep_removed.txt', '3-10-16_rep_removed.txt', '3-3-16_rep_removed.txt', '3-6-16_dem_removed.txt', '3-9-16_dem_removed.txt', '4-14-16_dem_removed.txt', '8-6-15_rep_removed.txt', '9-16-15_rep_removed.txt', '9-26-16_removed.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/Users/Paige/Documents/Data_Science/2016-Election-Project/data/Debates/transcripts/removed/')\n",
    "files = glob.glob(\"*.txt\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I'm creating a list where each entry in the list is a transcript\n",
    "transcripts = []\n",
    "for f in files:\n",
    "    fi = open(f, 'r')\n",
    "    txt = fi.read()\n",
    "    fi.close\n",
    "    transcripts.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAVUTO: It is 9:00 p.m. here at the North Charleston Coliseum and Performing Arts Center in South Carolina. Welcome to the sixth Republican presidential of the 2016 campaign, here on the Fox Business \n"
     ]
    }
   ],
   "source": [
    "print(transcripts[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's start out with one transcript and see what we can get working. I plan on trying NLTK's nltk.ne_chunk_sents() to do annotation. I don't know how well it will work for conversational data in this format, but I'll try it out because it's a simple place to start.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUOMO: All right. We are live at Drake University in Des Moines, Iowa, to hear from the Democratic presidential candidates and the people who matter most in this election, the voters.\n",
      "\n",
      "Welcome to our viewers in the United States and around the world, and, of course, here in Iowa, where we're being seen on our CNN affiliates across the state.\n",
      "\n",
      "We also want to welcome our servicemen and -women who are watching on the American Forces Network around the world, and to our listeners on the Westwood On\n"
     ]
    }
   ],
   "source": [
    "debate1 = transcripts[2]\n",
    "print(debate1[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It is 9:00 p.m. here at the North Charleston Coliseum and Performing Arts Center in South Carolina. Welcome to the sixth Republican presidential of the 2016 campaign, here on the Fox Business Network. I'm Neil Cavuto, alongside my friend and co-moderator Maria Bartiromo.\n",
      "\n",
      " Tonight we are working with Facebook to ask the candidates the questions voters want answered. And according to Facebook, the U.S. election has dominated the global conversation, with 131 million people talking about the 2016\n"
     ]
    }
   ],
   "source": [
    "removed_labels = []\n",
    "\n",
    "for txt in transcripts:\n",
    "    #Take care of all other speakers, labels\n",
    "    txt = re.sub(r\"([A-Z]+):\", r\"\", txt)\n",
    "    removed_labels.append(txt)\n",
    "print(removed_labels[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' It is 9:00 p.m. here at the North Charleston Coliseum and Performing Arts Center in South Carolina.', 'Welcome to the sixth Republican presidential of the 2016 campaign, here on the Fox Business Network.', \"I'm Neil Cavuto, alongside my friend and co-moderator Maria Bartiromo.\", 'Tonight we are working with Facebook to ask the candidates the questions voters want answered.', 'And according to Facebook, the U.S. election has dominated the global conversation, with 131 million people talking about the 2016 race.', 'That makes it the number one issue talked about on Facebook last year worldwide.', 'Now, the seven candidates on the stage tonight were selected based on their standing in six national polls, as well as polls in the early-voting states of Iowa and New Hampshire, those standings determining the position on the stage of the candidates tonight.', 'And here they are.', 'Businessman Donald Trump.', '[applause]\\n\\nTexas senator Ted Cruz.', '[applause]\\n\\nFlorida senator Marco Rubio.', '[applause]\\n\\nNeurosurgeon Ben Carson.', '[applause]\\n\\nNew Jersey governor Chris Christie.', '[applause]\\n\\nFormer Florida governor Jeb Bush.', 'And Ohio governor John Kasich.', \"[applause]\\n\\n Tonight's rules are simple: up to 90 seconds for each answer, one minute for each follow-up response.\", \"And if a candidate goes over the allotted time, you'll hear this.\", \"[bell rings] So let's get started.\", 'Candidates, jobs and growth â€” two of the biggest issues facing the country right now.', 'In his State of the Union address earlier this week, the president said, quote, \"we have the strongest, most durable economy in the world.\"']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentence tokenization\n",
    "sents = [nltk.sent_tokenize(transcript) for transcript in removed_labels]\n",
    "#Those [applause]\\n\\n might cause problems later. I will have to check on those.\n",
    "sents[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['It', 'is', '9:00', 'p.m.', 'here', 'at', 'the', 'North', 'Charleston', 'Coliseum', 'and', 'Performing', 'Arts', 'Center', 'in', 'South', 'Carolina', '.'], ['Welcome', 'to', 'the', 'sixth', 'Republican', 'presidential', 'of', 'the', '2016', 'campaign', ',', 'here', 'on', 'the', 'Fox', 'Business', 'Network', '.'], ['I', \"'m\", 'Neil', 'Cavuto', ',', 'alongside', 'my', 'friend', 'and', 'co-moderator', 'Maria', 'Bartiromo', '.'], ['Tonight', 'we', 'are', 'working', 'with', 'Facebook', 'to', 'ask', 'the', 'candidates', 'the', 'questions', 'voters', 'want', 'answered', '.'], ['And', 'according', 'to', 'Facebook', ',', 'the', 'U.S.', 'election', 'has', 'dominated', 'the', 'global', 'conversation', ',', 'with', '131', 'million', 'people', 'talking', 'about', 'the', '2016', 'race', '.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word tokenization in each sentence\n",
    "sent_toks = []\n",
    "transcript_toks = []\n",
    "\n",
    "for transcript in sents:\n",
    "    for sent in transcript:\n",
    "        transcript_toks.append(nltk.word_tokenize(sent))\n",
    "    sent_toks.append(transcript_toks)\n",
    "    transcript_toks = []\n",
    "\n",
    "sent_toks[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'), ('is', 'VBZ'), ('9:00', 'CD'), ('p.m.', 'NN'), ('here', 'RB'), ('at', 'IN'), ('the', 'DT'), ('North', 'NNP'), ('Charleston', 'NNP'), ('Coliseum', 'NNP'), ('and', 'CC'), ('Performing', 'NNP'), ('Arts', 'NNP'), ('Center', 'NNP'), ('in', 'IN'), ('South', 'NNP'), ('Carolina', 'NNP'), ('.', '.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(sent_toks[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('It', 'PRP'), ('is', 'VBZ'), ('9:00', 'CD'), ('p.m.', 'NN'), ('here', 'RB'), ('at', 'IN'), ('the', 'DT'), ('North', 'NNP'), ('Charleston', 'NNP'), ('Coliseum', 'NNP'), ('and', 'CC'), ('Performing', 'NNP'), ('Arts', 'NNP'), ('Center', 'NNP'), ('in', 'IN'), ('South', 'NNP'), ('Carolina', 'NNP'), ('.', '.')], [('Welcome', 'VB'), ('to', 'TO'), ('the', 'DT'), ('sixth', 'JJ'), ('Republican', 'NNP'), ('presidential', 'NN'), ('of', 'IN'), ('the', 'DT'), ('2016', 'CD'), ('campaign', 'NN'), (',', ','), ('here', 'RB'), ('on', 'IN'), ('the', 'DT'), ('Fox', 'NNP'), ('Business', 'NNP'), ('Network', 'NNP'), ('.', '.')], [('I', 'PRP'), (\"'m\", 'VBP'), ('Neil', 'JJ'), ('Cavuto', 'NNP'), (',', ','), ('alongside', 'IN'), ('my', 'PRP$'), ('friend', 'NN'), ('and', 'CC'), ('co-moderator', 'NN'), ('Maria', 'NNP'), ('Bartiromo', 'NNP'), ('.', '.')], [('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('working', 'VBG'), ('with', 'IN'), ('Facebook', 'NNP'), ('to', 'TO'), ('ask', 'VB'), ('the', 'DT'), ('candidates', 'NNS'), ('the', 'DT'), ('questions', 'NNS'), ('voters', 'NNS'), ('want', 'VBP'), ('answered', 'VBN'), ('.', '.')], [('And', 'CC'), ('according', 'VBG'), ('to', 'TO'), ('Facebook', 'NNP'), (',', ','), ('the', 'DT'), ('U.S.', 'NNP'), ('election', 'NN'), ('has', 'VBZ'), ('dominated', 'VBN'), ('the', 'DT'), ('global', 'JJ'), ('conversation', 'NN'), (',', ','), ('with', 'IN'), ('131', 'CD'), ('million', 'CD'), ('people', 'NNS'), ('talking', 'VBG'), ('about', 'IN'), ('the', 'DT'), ('2016', 'CD'), ('race', 'NN'), ('.', '.')]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POS tagging for each sentence\n",
    "sent_pos = []\n",
    "transcript_pos = []\n",
    "\n",
    "for transcript in sent_toks:\n",
    "    for sent in transcript:\n",
    "        transcript_pos.append(nltk.pos_tag(sent))\n",
    "    sent_pos.append(transcript_pos)\n",
    "    transcript_ = []\n",
    "\n",
    "sent_pos[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Named Entity Chunking\n",
    "sent_NER = []\n",
    "transcripts_NER = []\n",
    "for transcript in sent_pos:\n",
    "    for sent in sent_pos:\n",
    "        sent_NER.append(nltk.ne_chunk_sents(sent))\n",
    "    transcripts_NER.append(sent_NER)\n",
    "    sent_NER = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "f = open('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(transcripts_NER[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd1chunked_sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c06ba5b27cfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md1chunked_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1chunked_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0md1chunked_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd1chunked_sents' is not defined"
     ]
    }
   ],
   "source": [
    "d1chunked_sents = list(d1chunked_sents)\n",
    "d1chunked_sents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It didn't get all of Secretary Clinton as the PERSON...just Clinton\n",
    "print(d1chunked_sents[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sent in d1chunked_sents[:50]:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This does a nice job of pulling out the NE's, but it doesn't tell us where they're located in the text unless you look at the trees...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(d1chunked_sents[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1chunked_sents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1chunked_sents[3].set_label('HOLT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1chunked_sents[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It's probably not smart to get rid of S as the head label... but maybe it would be helpful to change it to the speaker? There doesn't seem to be a way for me to link the NE's either.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section is with help from a datacamp tutorial\n",
    "#https://campus.datacamp.com/courses/natural-language-processing-fundamentals-in-python/named-entity-recognition?ex=3\n",
    "\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in d1chunked_sents:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(l) for l in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people = []\n",
    "for sent in d1chunked_sents:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            if chunk.label() == 'PERSON':\n",
    "                people.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpe = []\n",
    "for sent in d1chunked_sents:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            if chunk.label() == 'GPE':\n",
    "                gpe.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "org = []\n",
    "for sent in d1chunked_sents:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            if chunk.label() == 'ORGANIZATION':\n",
    "                org.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "facility = []\n",
    "for sent in d1chunked_sents:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            if chunk.label() == 'FACILITY':\n",
    "                facility.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(facility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in facility:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in org:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people[1].leaves()[0][0]\n",
    "words = [leaf[0] for leaf in people[1].leaves()]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people_names = []\n",
    "name = ''\n",
    "for tree in people:\n",
    "    for leaf in tree.leaves():\n",
    "        name+=' '+str(leaf[0])\n",
    "    people_names.append(name.strip())\n",
    "    name = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(people_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Secretary' in people_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Secretary Clinton' in debate1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Secretary Clinton' in people_names\n",
    "#In fact it looks like it never recognizes Secretary Clinton as a PERSON. Just 'Clinton'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(debate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents[:50]:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(doc.ents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents[0].end_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents[0].ent_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents[0].label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents[0].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(doc.ents[0].subtree)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div class=\"entities\">I will ask you that question first, Secretary <mark data-entity=\"person\">Clinton</mark>.</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
